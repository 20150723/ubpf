// vi: ft=c
/*
 * Copyright 2015 Big Switch Networks, Inc
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#define _GNU_SOURCE
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <stdbool.h>
#include <unistd.h>
#include <inttypes.h>
#include <sys/mman.h>
#include <errno.h>
#include "ubpf_int.h"

#define DASM_CHECKS
#include <dasm_proto.h>
#include <dasm_x86.h>

|.arch x64

/* The hardcoded dasm_State variable name is confusing given our use of 'dst' */
#define dasm Dst

/*
 * DynASM can only dynamically address r0-r7.
 *
 * When a register above 5 is used, we copy it into r6 or r7 and then pass the
 * "bounce register" to the actual instruction. If it was the dst register then
 * we copy it back.
 *
 * CPU register r0-r5 maps to eBPF registers 0-5 directly.
 * r6: dst bounce register.
 * r7: src bounce register.
 * r8: eBPF register 6.
 * r9: eBPF register 7.
 * r10: eBPF register 8.
 * r11: eBPF register 9.
 * r12: eBPF register 10.
 */
#define NUM_DIRECT_REGS 6
#define BOUNCE_DST 6
#define BOUNCE_SRC 7
|.define BOUNCE_DST, r6
|.define BOUNCE_SRC, r7
|.define R6, r8
|.define R7, r9
|.define R8, r10
|.define R9, r11
|.define R10, r12

static int bounce_dst(dasm_State **dasm, int dst);
static int bounce_src(dasm_State **dasm, int src);
static void bounce_dst_back(dasm_State **dasm, int dst);

|.actionlist actions

ubpf_jit_fn
ubpf_compile(struct ubpf_vm *vm, char **errmsg)
{
    dasm_State *d;
    void *jitted = NULL;
    size_t jitted_size;
    int ret;

    *errmsg = NULL;

    if (vm->jitted) {
        return vm->jitted;
    }

    |.section code
    dasm_init(&d, DASM_MAXSECTION);
    dasm_setup(&d, actions);

    |.globals lbl_
    void *labels[lbl__MAX];
    dasm_setupglobal(&d, labels, lbl__MAX);

    dasm_growpc(&d, vm->num_insts);

    dasm_State **dasm = &d;

    /* Prologue */
    |.code
    |->entry:
    /* rbx (aka r3), rbp (aka r5), and r12 are callee save */
    | push r3
    | push r5
    | push r12

    /* First argument passed in rdi (aka r7) */
    | mov r1, r7

    int i;
    for (i = 0; i < vm->num_insts; i++) {
        struct ebpf_inst inst = vm->insts[i];
        |=>i:

        int src = bounce_src(dasm, inst.src);
        int dst = bounce_dst(dasm, inst.dst);
        int jmp_target = i + inst.offset + 1;

        switch (inst.opcode) {
        case EBPF_OP_ADD_IMM:
            | add Rd(dst), inst.imm
            break;
        case EBPF_OP_ADD_REG:
            | add Rd(dst), Rd(src)
            break;
        case EBPF_OP_SUB_IMM:
            | sub Rd(dst), inst.imm
            break;
        case EBPF_OP_SUB_REG:
            | sub Rd(dst), Rd(src)
            break;
#if 0
        case EBPF_OP_MUL_IMM:
            break;
        case EBPF_OP_MUL_REG:
            break;
        case EBPF_OP_DIV_IMM:
            break;
        case EBPF_OP_DIV_REG:
            break;
#endif
        case EBPF_OP_OR_IMM:
            | or Rd(dst), inst.imm
            break;
        case EBPF_OP_OR_REG:
            | or Rd(dst), Rd(src)
            break;
        case EBPF_OP_AND_IMM:
            | and Rd(dst), inst.imm
            break;
        case EBPF_OP_AND_REG:
            | and Rd(dst), Rd(src)
            break;
        case EBPF_OP_LSH_IMM:
            | shl Rd(dst), inst.imm
            break;
#if 0
        case EBPF_OP_LSH_REG:
            break;
#endif
        case EBPF_OP_RSH_IMM:
            | shr Rd(dst), inst.imm
            break;
#if 0
        case EBPF_OP_RSH_REG:
            break;
#endif
        case EBPF_OP_NEG:
            | neg Rd(dst)
            break;
#if 0
        case EBPF_OP_MOD_IMM:
            break;
        case EBPF_OP_MOD_REG:
            break;
#endif
        case EBPF_OP_XOR_IMM:
            | xor Rd(dst), inst.imm
            break;
        case EBPF_OP_XOR_REG:
            | xor Rd(dst), Rd(src)
            break;
        case EBPF_OP_MOV_IMM:
            | mov Rd(dst), inst.imm
            break;
        case EBPF_OP_MOV_REG:
            | mov Rd(dst), Rd(src)
            break;
        case EBPF_OP_ARSH_IMM:
            | sar Rd(dst), inst.imm
            break;
#if 0
        case EBPF_OP_ARSH_REG:
            break;
#endif

        case EBPF_OP_LE:
            /* No-op */
            break;
        case EBPF_OP_BE:
            if (inst.imm == 16) {
                /* TODO */
                abort();
            } else if (inst.imm == 32) {
                | bswap Rd(dst)
            } else if (inst.imm == 64) {
                | bswap Rq(dst)
            }
            break;

        case EBPF_OP_ADD64_IMM:
            | add Rq(dst), inst.imm
            break;
        case EBPF_OP_ADD64_REG:
            | add Rq(dst), Rq(src)
            break;
        case EBPF_OP_SUB64_IMM:
            | sub Rq(dst), inst.imm
            break;
        case EBPF_OP_SUB64_REG:
            | sub Rq(dst), Rq(src)
            break;
#if 0
        case EBPF_OP_MUL64_IMM:
            break;
        case EBPF_OP_MUL64_REG:
            break;
        case EBPF_OP_DIV64_IMM:
            break;
        case EBPF_OP_DIV64_REG:
            break;
#endif
        case EBPF_OP_OR64_IMM:
            | or Rq(dst), inst.imm
            break;
        case EBPF_OP_OR64_REG:
            | or Rq(dst), Rq(src)
            break;
        case EBPF_OP_AND64_IMM:
            | and Rq(dst), inst.imm
            break;
        case EBPF_OP_AND64_REG:
            | and Rq(dst), Rq(src)
            break;
        case EBPF_OP_LSH64_IMM:
            | shl Rq(dst), inst.imm
            break;
#if 0
        case EBPF_OP_LSH64_REG:
            break;
#endif
        case EBPF_OP_RSH64_IMM:
            | shr Rq(dst), inst.imm
            break;
#if 0
        case EBPF_OP_RSH64_REG:
            break;
#endif
        case EBPF_OP_NEG64:
            | neg Rq(dst)
            break;
#if 0
        case EBPF_OP_MOD64_IMM:
            break;
        case EBPF_OP_MOD64_REG:
            break;
#endif
        case EBPF_OP_XOR64_IMM:
            | xor Rq(dst), inst.imm
            break;
        case EBPF_OP_XOR64_REG:
            | xor Rq(dst), Rq(src)
            break;
        case EBPF_OP_MOV64_IMM:
            /* TODO use shorter mov for smaller immediates */
            | mov Rq(dst), inst.imm
            break;
        case EBPF_OP_MOV64_REG:
            | mov Rq(dst), Rq(src)
            break;
        case EBPF_OP_ARSH64_IMM:
            | sar Rq(dst), inst.imm
            break;
#if 0
        case EBPF_OP_ARSH_REG:
            break;
#endif

        /* TODO MEM opcodes */

        case EBPF_OP_JA:
            | jmp =>jmp_target
            break;
        case EBPF_OP_JEQ_IMM:
            | cmp Rq(dst), inst.imm
            | je =>jmp_target
            break;
        case EBPF_OP_JEQ_REG:
            | cmp Rq(dst), Rq(src)
            | je =>jmp_target
            break;
        case EBPF_OP_JGT_IMM:
            | cmp Rq(dst), inst.imm
            | ja =>jmp_target
            break;
        case EBPF_OP_JGT_REG:
            | cmp Rq(dst), Rq(src)
            | ja =>jmp_target
            break;
        case EBPF_OP_JGE_IMM:
            | cmp Rq(dst), inst.imm
            | jae =>jmp_target
            break;
        case EBPF_OP_JGE_REG:
            | cmp Rq(dst), Rq(src)
            | jae =>jmp_target
            break;
        case EBPF_OP_JSET_IMM:
            | test Rq(dst), inst.imm
            | jnz =>jmp_target
            break;
        case EBPF_OP_JSET_REG:
            | test Rq(dst), Rq(src)
            | jnz =>jmp_target
            break;
        case EBPF_OP_JNE_IMM:
            | cmp Rq(dst), inst.imm
            | jne =>jmp_target
            break;
        case EBPF_OP_JNE_REG:
            | cmp Rq(dst), Rq(src)
            | jne =>jmp_target
            break;
        case EBPF_OP_JSGT_IMM:
            | cmp Rq(dst), inst.imm
            | jg =>jmp_target
            break;
        case EBPF_OP_JSGT_REG:
            | cmp Rq(dst), Rq(src)
            | jg =>jmp_target
            break;
        case EBPF_OP_JSGE_IMM:
            | cmp Rq(dst), inst.imm
            | jge =>jmp_target
            break;
        case EBPF_OP_JSGE_REG:
            | cmp Rq(dst), Rq(src)
            | jge =>jmp_target
            break;
        case EBPF_OP_EXIT:
            /* TODO omit if last instruction */
            | jmp ->exit
            break;

        default:
            *errmsg = ubpf_error("unknown opcode 0x%02x at PC %d", inst.opcode, i);
            goto error;
        }

        bounce_dst_back(dasm, inst.dst);
    }

    /* Epilogue */
    | ->exit:
    | pop r12
    | pop r5
    | pop r3
    | ret

    if ((ret = dasm_link(dasm, &jitted_size)) != DASM_S_OK) {
        *errmsg = ubpf_error("internal uBPF error: dasm_link failed: %d", ret);
        goto error;
    }

    jitted = mmap(0, jitted_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (jitted == MAP_FAILED) {
        *errmsg = ubpf_error("internal uBPF error: mmap failed: %s\n", strerror(errno));
        goto error;
    }

    if ((ret = dasm_encode(dasm, jitted)) != DASM_S_OK) {
        *errmsg = ubpf_error("internal uBPF error: dasm_encode failed: %d\n", ret);
        goto error;
    }

    if (mprotect(jitted, jitted_size, PROT_READ | PROT_EXEC) < 0) {
        *errmsg = ubpf_error("internal uBPF error: mprotect failed: %s\n", strerror(errno));
        goto error;
    }

    dasm_free(dasm);
    vm->jitted = labels[lbl_entry];
    vm->jitted_size = jitted_size;
    return vm->jitted;

error:
    dasm_free(dasm);
    if (jitted) {
        munmap(jitted, jitted_size);
    }
    return NULL;
}

static int
bounce_dst(dasm_State **dasm, int dst)
{
    /* TODO omit MOV if dst is not read (mov, lddw, ...) */
    if (dst >= NUM_DIRECT_REGS) {
        if (dst == 6) {
            | mov BOUNCE_DST, R6
        } else if (dst == 7) {
            | mov BOUNCE_DST, R7
        } else if (dst == 8) {
            | mov BOUNCE_DST, R8
        } else if (dst == 9) {
            | mov BOUNCE_DST, R9
        } else if (dst == 10) {
            | mov BOUNCE_DST, R10
        } else {
            abort();
        }
        return BOUNCE_DST;
    } else {
        return dst;
    }
}

static int
bounce_src(dasm_State **dasm, int src)
{
    if (src >= NUM_DIRECT_REGS) {
        if (src == 6) {
            | mov BOUNCE_SRC, R6
        } else if (src == 7) {
            | mov BOUNCE_SRC, R7
        } else if (src == 8) {
            | mov BOUNCE_SRC, R8
        } else if (src == 9) {
            | mov BOUNCE_SRC, R9
        } else if (src == 10) {
            | mov BOUNCE_SRC, R10
        } else {
            abort();
        }
        return BOUNCE_SRC;
    } else {
        return src;
    }
}

static void
bounce_dst_back(dasm_State **dasm, int dst)
{
    /* TODO omit if dst is not written to (j*, st*, ...) */
    if (dst == 6) {
        | mov R6, BOUNCE_DST
    } else if (dst == 7) {
        | mov R7, BOUNCE_DST
    } else if (dst == 8) {
        | mov R8, BOUNCE_DST
    } else if (dst == 9) {
        | mov R9, BOUNCE_DST
    } else if (dst == 10) {
        | mov R10, BOUNCE_DST
    }
}
